{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datashading a 2.7-billion-point Open Street Map database\n",
    "\n",
    "Most [datashader](https://github.com/bokeh/datashader) examples use \"medium-sized\" datasets, because they need to be small enough to be distributed over the internet without racking up huge bandwidth charges for the project maintainers.  But given that Datashader supports [Dask](http://dask.pydata.org) dataframes, Datashader can easily be used with truly large datasets.  Such datasets can be much larger than the available amount of physical memory on the machine, either by paging in data on a single machine, or by distributing data across multiple machines. Here we illustrate how to work \"out of core\" on a single machine.\n",
    "\n",
    "This example will use a 2.7-billion point dataset, which is unfortunately too large to distribute with Datashader (7GB compressed). The data is taken from Open Street Map's (OSM) [bulk GPS point data](https://blog.openstreetmap.org/2012/04/01/bulk-gps-point-data/). The data was collected by OSM contributors' GPS devices, and was provided as a CSV file of `latitude,longitude` coordinates. The data was downloaded from their website, extracted, converted to use positions in Web Mercator format using `datashader.utils.lnglat_to_meters()`, and then stored in a [parquet](https://github.com/dask/fastparquet) file for [faster disk access](https://github.com/bokeh/datashader/issues/129#issuecomment-300515690). To run this notebook, you would need to do the same process yourself to obtain `osm.snappy.parq`.   Once you have it, you can follow the steps below to load and plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask.diagnostics as diag\n",
    "import datashader as ds\n",
    "import datashader.transfer_functions as tf\n",
    "\n",
    "from functools import partial\n",
    "from colorcet import fire\n",
    "from datashader.utils import export_image\n",
    "from datashader.colors import colormap_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.io.parquet.read_parquet('data/osm.snappy.parq')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation\n",
    "\n",
    "First, we create a canvas to provide pixel-shaped bins in which points can be aggregated, and then aggregate the data to produce a fixed-size aggregate array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound = 20026376.39\n",
    "bounds = dict(x_range = (-bound, bound), y_range = (int(-bound*0.4), int(bound*0.6)))\n",
    "plot_width = 1000\n",
    "plot_height = int(plot_width*0.5)\n",
    "\n",
    "cvs = ds.Canvas(plot_width=plot_width, plot_height=plot_height, **bounds)\n",
    "\n",
    "with diag.ProgressBar(), diag.Profiler() as prof, diag.ResourceProfiler(0.5) as rprof:\n",
    "    agg = cvs.points(df, 'x', 'y', ds.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Function\n",
    "\n",
    "Next, we create an image out of the aggregate array, by mapping small (but nonzero) counts to light blue, the largest counts to dark blue, and interpolating according to a logarithmic function in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.shade(agg, cmap=[\"lightblue\", \"darkblue\"], how='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's some odd, low-count, nearly-uniform noise going on in the tropics. It's worth trying to figure out what that could be, but for now we can filter it out quickly from the aggregated data using the `where` method, and switch to the parameter-free histogram equalization method for mapping to colors using various colormaps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.shade(agg.where(agg > 15), cmap=[\"lightblue\", \"darkblue\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export = partial(export_image, export_path=\"export\", background=\"black\")\n",
    "\n",
    "export(tf.shade(agg.where(agg > 15), cmap=[\"darkblue\", \"lightcyan\"]), \"OSM_black_blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export(tf.shade(agg.where(agg > 15), cmap=fire), \"OSM_fire\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a decent map of world population, with Europe apparently having particularly many OpenStreetMap contributors. The long great-circle paths are presumably flight or boat trips, from devices that log their GPS coordinates more than 15 times during the space of one pixel in this plot (or else they would have been elimnated by the `where` call)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Profile\n",
    "\n",
    "Dask offers some tools to visualize how memory and processing power are being used during these calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook\n",
    "from bokeh.resources import CDN\n",
    "output_notebook(CDN, hide_banner=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag.visualize([prof, rprof])\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance notes:\n",
    "- On a 16GB machine, most of the time is spent reading the data from disk (the purple rectangles)\n",
    "- Reading time includes not just disk I/O, but decompressing chunks of data\n",
    "- The disk reads don't release the [Global Interpreter Lock](https://wiki.python.org/moin/GlobalInterpreterLock) (GIL), and so CPU usage (see second chart above) drops to only one core during those periods.\n",
    "- During the aggregation steps (the green rectangles), CPU usage on this machine with 8 hyperthreaded cores (4 full cores) spikes to nearly 800%, because the aggregation function is implemented in parallel. \n",
    "- The data takes up 11 GB of memory when uncompressed, but only a peak of around 6 GB of physical memory is ever used because the data is paged in as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive plotting\n",
    "\n",
    "If you have enough RAM to hold the whole dataset (or are very patient), you can uncomment the `InteractiveImage` line below and run the cell to build an interactive plot where you can select a region for zooming. Without enough RAM, computation has to be done out of core, and it could take a half-minute or so to process a pan or zoom event before the plot gets updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_notebook\n",
    "from bokeh.io import push_notebook\n",
    "from datashader.bokeh_ext import InteractiveImage\n",
    "from datashader import transfer_functions as tf\n",
    "\n",
    "def create_image(x_range, y_range, w, h):\n",
    "    cvs = ds.Canvas(x_range=x_range, y_range=y_range)\n",
    "    agg = cvs.points(df, 'x', 'y', ds.count())\n",
    "    return tf.shade(agg.where(agg > 20), cmap=[\"lightcyan\", \"darkblue\"], how=\"log\")\n",
    "\n",
    "p = figure(tools='pan,wheel_zoom,box_zoom,reset', plot_width=plot_width, plot_height=plot_height, **bounds)\n",
    "           \n",
    "p.axis.visible = False\n",
    "p.xgrid.grid_line_color = None\n",
    "p.ygrid.grid_line_color = None\n",
    "#InteractiveImage(p, create_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
